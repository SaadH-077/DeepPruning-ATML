{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXngeurLTl_v"
      },
      "source": [
        "# Task 2: Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2Xx4fdMTpOc"
      },
      "source": [
        "#### Installing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODCs9uB9TrYt",
        "outputId": "d8a4a215-039d-410d-8c05-bc0aebf6d860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "Collecting torchao\n",
            "  Downloading torchao-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading torchao-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchao\n",
            "Successfully installed torchao-0.6.1\n",
            "Collecting optimum-quanto\n",
            "  Downloading optimum_quanto-0.2.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from optimum-quanto) (2.5.0+cu121)\n",
            "Collecting ninja (from optimum-quanto)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optimum-quanto) (1.26.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from optimum-quanto) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from optimum-quanto) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->optimum-quanto) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->optimum-quanto) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->optimum-quanto) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->optimum-quanto) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->optimum-quanto) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.4.0->optimum-quanto) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.4.0->optimum-quanto) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->optimum-quanto) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->optimum-quanto) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->optimum-quanto) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->optimum-quanto) (4.66.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.4.0->optimum-quanto) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->optimum-quanto) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->optimum-quanto) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->optimum-quanto) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->optimum-quanto) (2024.8.30)\n",
            "Downloading optimum_quanto-0.2.6-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.1/165.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ninja, optimum-quanto\n",
            "Successfully installed ninja-1.11.1.1 optimum-quanto-0.2.6\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "!pip install torchao\n",
        "!pip install optimum-quanto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h0Vt8UwTl_z"
      },
      "source": [
        "#### Loading in the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z7pLdyyuTl_z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import models\n",
        "from torchinfo import summary\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import copy\n",
        "from torchao.quantization.quant_api import (quantize_, int8_dynamic_activation_int8_weight, int4_weight_only, int8_weight_only)\n",
        "from optimum.quanto import quantize, qint4\n",
        "from torch.amp.autocast_mode import autocast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xMtMiPcUUbq"
      },
      "source": [
        "#### Mounting the drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Cqcaic0UWmC",
        "outputId": "e52894c3-f2bb-4307-eafd-749ae0909c74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agX89hs3M3j2"
      },
      "source": [
        "Setting the device to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OLbmTly7M4zy"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAoom-HdTl_1"
      },
      "source": [
        "#### Loading in the VGG-11 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YR1gb6XTl_1",
        "outputId": "18b148d4-c0f3-4d36-b1e8-efd52beee80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n",
            "100%|██████████| 507M/507M [00:02<00:00, 225MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 100]                  --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         1,792\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-4                       [1, 128, 112, 112]        73,856\n",
              "│    └─ReLU: 2-5                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-7                       [1, 256, 56, 56]          295,168\n",
              "│    └─ReLU: 2-8                         [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          590,080\n",
              "│    └─ReLU: 2-10                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-12                      [1, 512, 28, 28]          1,180,160\n",
              "│    └─ReLU: 2-13                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-14                      [1, 512, 28, 28]          2,359,808\n",
              "│    └─ReLU: 2-15                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-17                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-18                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-19                      [1, 512, 14, 14]          2,359,808\n",
              "│    └─ReLU: 2-20                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 100]                  --\n",
              "│    └─Linear: 2-22                      [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-23                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-24                     [1, 4096]                 --\n",
              "│    └─Linear: 2-25                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-26                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-27                     [1, 4096]                 --\n",
              "│    └─Linear: 2-28                      [1, 100]                  409,700\n",
              "==========================================================================================\n",
              "Total params: 129,176,036\n",
              "Trainable params: 129,176,036\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 7.61\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 59.47\n",
              "Params size (MB): 516.70\n",
              "Estimated Total Size (MB): 576.78\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Loading in VGG-11\n",
        "vgg11 = models.vgg11(weights= models.VGG11_Weights.DEFAULT)\n",
        "\n",
        "# Preparing the classifier layer for CIFAR-100 classification\n",
        "vgg11.classifier[6] = nn.Linear(4096, 100)\n",
        "\n",
        "# Summary of VGG-11\n",
        "summary(vgg11, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5MxeVnHUBgz"
      },
      "source": [
        "#### Loading in the CIFAR-100 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cg0HWOguUEmd",
        "outputId": "13e9c4bc-3354-4cda-9d11-01826df69109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "CIFAR-100 dataset loaded into ./data\n"
          ]
        }
      ],
      "source": [
        "# data_dir = '/content/drive/MyDrive/ATML_PA3'\n",
        "data_dir = './data'\n",
        "\n",
        "# Define transformations (you can modify this as per your needs)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
        "])\n",
        "\n",
        "# Download and load the CIFAR-100 training set\n",
        "train_dataset = torchvision.datasets.CIFAR100(\n",
        "    root=data_dir,  # Path where the data should be downloaded\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Download and load the CIFAR-100 test set\n",
        "test_dataset = torchvision.datasets.CIFAR100(\n",
        "    root=data_dir,  # Path where the data should be downloaded\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(\"CIFAR-100 dataset loaded into\", data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I_YrqUxkCew"
      },
      "source": [
        "### Finetuning the Model for 5 epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE37c6XckCex"
      },
      "source": [
        "#### Freezing the classifier backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzrTEdk8kCex",
        "outputId": "2ee5df16-139f-48ac-d1f2-ec7fb179b69c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 100]                  --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         (1,792)\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-4                       [1, 128, 112, 112]        (73,856)\n",
              "│    └─ReLU: 2-5                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-7                       [1, 256, 56, 56]          (295,168)\n",
              "│    └─ReLU: 2-8                         [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          (590,080)\n",
              "│    └─ReLU: 2-10                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-12                      [1, 512, 28, 28]          (1,180,160)\n",
              "│    └─ReLU: 2-13                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-14                      [1, 512, 28, 28]          (2,359,808)\n",
              "│    └─ReLU: 2-15                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-17                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-18                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-19                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-20                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 100]                  --\n",
              "│    └─Linear: 2-22                      [1, 4096]                 (102,764,544)\n",
              "│    └─ReLU: 2-23                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-24                     [1, 4096]                 --\n",
              "│    └─Linear: 2-25                      [1, 4096]                 (16,781,312)\n",
              "│    └─ReLU: 2-26                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-27                     [1, 4096]                 --\n",
              "│    └─Linear: 2-28                      [1, 100]                  409,700\n",
              "==========================================================================================\n",
              "Total params: 129,176,036\n",
              "Trainable params: 409,700\n",
              "Non-trainable params: 128,766,336\n",
              "Total mult-adds (G): 7.61\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 59.47\n",
              "Params size (MB): 516.70\n",
              "Estimated Total Size (MB): 576.78\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Freezing the model backbone\n",
        "for param in vgg11.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replacing the classifier head for CIFAR-100 classification\n",
        "vgg11.classifier[6] = nn.Linear(4096, 100)\n",
        "\n",
        "# Unfreezing the last layer in the classifier head\n",
        "for param in vgg11.classifier[6].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Confirming the model is unfrozen\n",
        "summary(vgg11, input_size=(1, 3, 224, 224))\n",
        "\n",
        "# trainable_params = sum(p.numel() for p in vgg11.parameters() if p.requires_grad)\n",
        "# print(f\"Trainable parameters: {trainable_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG7pvuvokCex"
      },
      "source": [
        "#### Finetuning the modified model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU8tdJ6qkCex",
        "outputId": "253fb2a2-6f37-418c-8174-8cda26e58a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [03:05<00:00,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 1.9600570601270633, Training Accuracy: 47.622\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:37<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Test Loss: 1.4580630572738162, Test Accuracy: 58.34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [03:07<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training Loss: 1.6380248533947694, Training Accuracy: 54.71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:37<00:00,  4.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Test Loss: 1.382804438566706, Test Accuracy: 60.32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [03:05<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training Loss: 1.5717946741434619, Training Accuracy: 56.586\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:37<00:00,  4.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Test Loss: 1.367748185327858, Test Accuracy: 61.53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [03:05<00:00,  4.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training Loss: 1.5450494482998958, Training Accuracy: 57.578\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:37<00:00,  4.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Test Loss: 1.396505104888017, Test Accuracy: 60.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [03:07<00:00,  4.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training Loss: 1.5171778877372937, Training Accuracy: 58.438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:37<00:00,  4.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Test Loss: 1.3678201175039741, Test Accuracy: 61.85\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Finetuning the model on the CIFAR-100 dataset for 3 epochs\n",
        "vgg11.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vgg11.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vgg11.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for i, data in enumerate(tqdm(train_loader)):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vgg11(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "    train_losses.append(running_loss / len(train_loader))\n",
        "    train_acc.append(100 * correct / total)\n",
        "    print(f\"Epoch {epoch + 1}, Training Loss: {running_loss / len(train_loader)}, Training Accuracy: {100 * correct / total}\")\n",
        "\n",
        "    vgg11.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(tqdm(test_loader)):\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = vgg11(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "        test_losses.append(running_loss / len(test_loader))\n",
        "        print(f\"Epoch {epoch + 1}, Test Loss: {running_loss / len(test_loader)}, Test Accuracy: {100 * correct / total}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W36AeSdQkCey"
      },
      "source": [
        "#### Saving the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvUkTDyAkCey",
        "outputId": "4879aa1c-2e76-499a-cff0-8118189b26a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/ATML_PA3/vgg11_cifar100.pth\n"
          ]
        }
      ],
      "source": [
        "# Saving the model\n",
        "save_dir = '/content/drive/MyDrive/ATML_PA3/'  # Example directory, replace as needed\n",
        "save_path = os.path.join(save_dir, 'vgg11_cifar100.pth')\n",
        "torch.save(vgg11, save_path)\n",
        "print(f\"Model saved to {save_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL4SxfC2kCey"
      },
      "source": [
        "#### Plotting the testing and training losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "0pTCpRlHkCey",
        "outputId": "c282c86b-bc0c-4090-d6a1-e8d4e1ed7aea"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotting the training and test losses over epochs\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(train_losses, label=\"Training Loss\")\n",
        "plt.plot(test_losses, label=\"Test Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading in the model using the pth file"
      ],
      "metadata": {
        "id": "ibxLtXEZMaWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model_path = '/content/drive/MyDrive/vgg11_cifar100.pth'\n",
        "finetuned_model = torch.load(model_path)\n",
        "# finetuned_model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovt1uq9RMcP3",
        "outputId": "63bf5938-ad33-4135-d0b2-e2bf95cdb71a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-bfc888ea7759>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  finetuned_model = torch.load(model_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idMGzKKdVyAg"
      },
      "source": [
        "### Post Training Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI-NEc-5V0fk"
      },
      "source": [
        "#### PTQ with FP16 bit width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35wupZx2V6HT",
        "outputId": "cc99cc4e-d88b-4d22-9940-18a30e26eb63"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 100]                  --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         (1,792)\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-4                       [1, 128, 112, 112]        (73,856)\n",
              "│    └─ReLU: 2-5                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-7                       [1, 256, 56, 56]          (295,168)\n",
              "│    └─ReLU: 2-8                         [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          (590,080)\n",
              "│    └─ReLU: 2-10                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-12                      [1, 512, 28, 28]          (1,180,160)\n",
              "│    └─ReLU: 2-13                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-14                      [1, 512, 28, 28]          (2,359,808)\n",
              "│    └─ReLU: 2-15                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-17                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-18                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-19                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-20                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 100]                  --\n",
              "│    └─Linear: 2-22                      [1, 4096]                 (102,764,544)\n",
              "│    └─ReLU: 2-23                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-24                     [1, 4096]                 --\n",
              "│    └─Linear: 2-25                      [1, 4096]                 (16,781,312)\n",
              "│    └─ReLU: 2-26                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-27                     [1, 4096]                 --\n",
              "│    └─Linear: 2-28                      [1, 100]                  409,700\n",
              "==========================================================================================\n",
              "Total params: 129,176,036\n",
              "Trainable params: 409,700\n",
              "Non-trainable params: 128,766,336\n",
              "Total mult-adds (G): 7.61\n",
              "==========================================================================================\n",
              "Input size (MB): 0.30\n",
              "Forward/backward pass size (MB): 29.74\n",
              "Params size (MB): 258.35\n",
              "Estimated Total Size (MB): 288.39\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Quantizing with FP16 bit width by simply casting to this data type\n",
        "# Creating a deep copy of the model to keep the original intact\n",
        "# quantized_model_fp16 = copy.deepcopy(vgg11)\n",
        "quantized_model_fp16 = copy.deepcopy(finetuned_model)\n",
        "\n",
        "# Casting the model to float16\n",
        "quantized_model_fp16 = quantized_model_fp16.to(dtype=torch.float16)\n",
        "\n",
        "# Printing the model summary following quanitzation\n",
        "summary(quantized_model_fp16, input_size=(1, 3, 224, 224), dtypes=[torch.float16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ9x8rJ5alhG"
      },
      "source": [
        "#### Testing the quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1PbzegJanjV",
        "outputId": "0a2b21f4-9d6e-4c2c-9009-cfc01008a6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Accuracy: 61.85000000000001\n",
            "Quantized Accuracy: 61.88\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Evaluate the quantized model\n",
        "def evaluate_normal_vgg11(model, dataloader):\n",
        "    \"\"\"Evaluates the normal (float32) VGG11 model.\n",
        "\n",
        "    Args:\n",
        "        model: The normal VGG11 model.\n",
        "        dataloader: The dataloader for the evaluation data.\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model on the evaluation data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating Normal VGG11\", leave=False):\n",
        "            images = images.to(device)  # Move images to device\n",
        "            labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "\n",
        "            outputs = model(images)  # Perform forward pass\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * (correct / total)\n",
        "    return accuracy\n",
        "\n",
        "def evaluate_float16_vgg11(model, dataloader):\n",
        "    \"\"\"Evaluates the float16 quantized VGG11 model.\n",
        "\n",
        "    Args:\n",
        "        model: The float16 quantized VGG11 model.\n",
        "        dataloader: The dataloader for the evaluation data.\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model on the evaluation data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating Float16 VGG11\", leave=False):\n",
        "            images = images.to(device)  # Move images to device\n",
        "            labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "            images = images.to(dtype = torch.float16)  # Cast images to float16\n",
        "\n",
        "            outputs = model(images)  # Perform forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * (correct / total)\n",
        "    return accuracy\n",
        "\n",
        "def evaluate_model_float16_test(model, test_loader, criterion, device):\n",
        "\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            inputs = inputs.to(torch.float16)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Get predictions and accumulate them\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "            # Collect predictions and labels\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = 100 * total_correct / total_samples\n",
        "    average_loss = running_loss / len(test_loader)\n",
        "    print(f\"Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy, average_loss, all_preds, all_labels\n",
        "\n",
        "# Send models to the correct device\n",
        "quantized_model_fp16 = quantized_model_fp16.to(device)\n",
        "# vgg11 = vgg11.to(device)\n",
        "finetuned_model = finetuned_model.to(device)\n",
        "\n",
        "# Evaluating both of the models\n",
        "original_acc = evaluate_normal_vgg11(finetuned_model, test_loader)\n",
        "fp16_quantized_acc = evaluate_float16_vgg11(quantized_model_fp16, test_loader)\n",
        "\n",
        "print(\"Original Accuracy:\", original_acc)\n",
        "print(\"Quantized Accuracy:\", fp16_quantized_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpuZR1tc1GJU"
      },
      "source": [
        "#### QAT with the FP16 quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g27l1KbN1Kn_",
        "outputId": "d0ad84f6-e3b2-4679-9cc6-ece4240a3aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Average Loss: 1.3243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3], Average Loss: 1.2845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3], Average Loss: 1.2561\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Accuracy after QAT: 64.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# Move model to device\n",
        "quantized_model_fp16 = quantized_model_fp16.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(quantized_model_fp16.parameters(), lr=0.01)\n",
        "num_epochs = 3  # Adjust based on performance needs\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(quantized_model_fp16.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    quantized_model_fp16.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "        images, labels = images.to(device), labels.to(device)  # Only move to GPU\n",
        "\n",
        "        images = images.to(dtype = torch.float16)  # Cast images to float16\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = quantized_model_fp16(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        nn.utils.clip_grad_norm_(quantized_model_fp16.parameters(), max_norm=0.5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Final evaluation after fine-tuning\n",
        "final_accuracy = evaluate_float16_vgg11(quantized_model_fp16, test_loader)\n",
        "print(f\"Final Accuracy after QAT: {final_accuracy}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_jodRG5TIZW"
      },
      "source": [
        "#### PTQ with BF16 bit width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "intWKXc2TNqD",
        "outputId": "173dfd82-bfa3-4863-fcd4-0a6153ffc551"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 100]                  --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         (1,792)\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-4                       [1, 128, 112, 112]        (73,856)\n",
              "│    └─ReLU: 2-5                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-7                       [1, 256, 56, 56]          (295,168)\n",
              "│    └─ReLU: 2-8                         [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          (590,080)\n",
              "│    └─ReLU: 2-10                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-12                      [1, 512, 28, 28]          (1,180,160)\n",
              "│    └─ReLU: 2-13                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-14                      [1, 512, 28, 28]          (2,359,808)\n",
              "│    └─ReLU: 2-15                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-17                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-18                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-19                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-20                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 100]                  --\n",
              "│    └─Linear: 2-22                      [1, 4096]                 (102,764,544)\n",
              "│    └─ReLU: 2-23                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-24                     [1, 4096]                 --\n",
              "│    └─Linear: 2-25                      [1, 4096]                 (16,781,312)\n",
              "│    └─ReLU: 2-26                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-27                     [1, 4096]                 --\n",
              "│    └─Linear: 2-28                      [1, 100]                  409,700\n",
              "==========================================================================================\n",
              "Total params: 129,176,036\n",
              "Trainable params: 409,700\n",
              "Non-trainable params: 128,766,336\n",
              "Total mult-adds (G): 7.61\n",
              "==========================================================================================\n",
              "Input size (MB): 0.30\n",
              "Forward/backward pass size (MB): 29.74\n",
              "Params size (MB): 258.35\n",
              "Estimated Total Size (MB): 288.39\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Now casting the model to BF16 width for quantization\n",
        "# quantized_model_bf16 = copy.deepcopy(vgg11).to(dtype=torch.bfloat16)\n",
        "# quantized_model_bf16 = copy.deepcopy(vgg11)\n",
        "quantized_model_bf16 = copy.deepcopy(finetuned_model)\n",
        "quantized_model_bf16 = quantized_model_bf16.to(dtype=torch.bfloat16)\n",
        "\n",
        "# Printing the model summary following quanitzation\n",
        "summary(quantized_model_bf16, input_size=(1, 3, 224, 224), dtypes=[torch.bfloat16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNkrxunjT9OR"
      },
      "source": [
        "#### Testing the quantized BF16 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGV0Oo0JUAPo",
        "outputId": "2a092b9b-b98c-46de-c8fb-9d4092a0bb78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                            "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BF16 Quantized Accuracy: 61.980000000000004\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "def evaluate_bfloat16_vgg11(model, dataloader):\n",
        "    \"\"\"Evaluates the bfloat16 quantized VGG11 model.\n",
        "\n",
        "    Args:\n",
        "        model: The bfloat16 quantized VGG11 model.\n",
        "        dataloader: The dataloader for the evaluation data.\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model on the evaluation data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating BFloat16 VGG11\", leave=False):\n",
        "            images = images.to(device)  # Move images to device\n",
        "            labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "            # Cast images to bfloat16\n",
        "            images = images.bfloat16()\n",
        "\n",
        "            outputs = model(images)  # Perform forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * (correct / total)\n",
        "    return accuracy\n",
        "\n",
        "# Testing with the bf16 quantized model\n",
        "bf16_quantized_acc = evaluate_bfloat16_vgg11(quantized_model_bf16, test_loader)\n",
        "print(\"BF16 Quantized Accuracy:\", bf16_quantized_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6tbfcz3GbNM"
      },
      "source": [
        "#### QAT with the BF16 quantized model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v1Proa2Gdgi",
        "outputId": "29d896f0-fe8c-47f3-be77-d7b548f731ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Average Loss: 1.4064, Training Accuracy: 60.54%\n",
            "Epoch [1/3], Test Accuracy: 61.98%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3], Average Loss: 1.4013, Training Accuracy: 60.64%\n",
            "Epoch [2/3], Test Accuracy: 61.98%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3], Average Loss: 1.4062, Training Accuracy: 60.69%\n",
            "Epoch [3/3], Test Accuracy: 61.98%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Move model to device without explicit FP16 casting\n",
        "quantized_model_bf16 = quantized_model_bf16.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(quantized_model_bf16.parameters(), lr=1e-7)\n",
        "optimizer = optim.SGD(quantized_model_fp16.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "num_epochs = 3  # Adjust based on performance needs\n",
        "for epoch in range(num_epochs):\n",
        "    quantized_model_bf16.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "        images, labels = images.to(device), labels.to(device)  # Only move to GPU\n",
        "\n",
        "        # Convert images to bfloat16 format\n",
        "        images = images.bfloat16()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = quantized_model_bf16(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        nn.utils.clip_grad_norm_(quantized_model_bf16.parameters(), max_norm=0.5)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy for this batch\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Calculate and print the average loss and training accuracy for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Evaluate on the test set and print test accuracy\n",
        "    quantized_model_bf16.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images = images.bfloat16()\n",
        "            outputs = quantized_model_bf16(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_DokBZ9p-ML"
      },
      "source": [
        "PTQ with INT8 Bit Width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf_nt140p_3h",
        "outputId": "3cf54314-db89-4ec4-9858-3c1a7f064456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 100]                  --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         (1,792)\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-4                       [1, 128, 112, 112]        (73,856)\n",
              "│    └─ReLU: 2-5                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-7                       [1, 256, 56, 56]          (295,168)\n",
              "│    └─ReLU: 2-8                         [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          (590,080)\n",
              "│    └─ReLU: 2-10                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-12                      [1, 512, 28, 28]          (1,180,160)\n",
              "│    └─ReLU: 2-13                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-14                      [1, 512, 28, 28]          (2,359,808)\n",
              "│    └─ReLU: 2-15                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-17                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-18                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-19                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-20                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 100]                  --\n",
              "│    └─Linear: 2-22                      [1, 4096]                 (102,764,544)\n",
              "│    └─ReLU: 2-23                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-24                     [1, 4096]                 --\n",
              "│    └─Linear: 2-25                      [1, 4096]                 (16,781,312)\n",
              "│    └─ReLU: 2-26                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-27                     [1, 4096]                 --\n",
              "│    └─Linear: 2-28                      [1, 100]                  409,700\n",
              "==========================================================================================\n",
              "Total params: 129,176,036\n",
              "Trainable params: 100\n",
              "Non-trainable params: 129,175,936\n",
              "Total mult-adds (G): 7.61\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 59.47\n",
              "Params size (MB): 516.70\n",
              "Estimated Total Size (MB): 576.78\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Quantizing the model to int 8 bit width using the torch ao apis\n",
        "# int8_quantized_model = copy.deepcopy(vgg11)\n",
        "int8_quantized_model = copy.deepcopy(finetuned_model)\n",
        "\n",
        "# Quantizing the model\n",
        "quantize_(int8_quantized_model, int8_weight_only())\n",
        "\n",
        "# Printing the model summary following quanitzation\n",
        "summary(int8_quantized_model, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqdmWIw9kCe3"
      },
      "source": [
        "#### Evaluating the model after quantization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int8_quantized_model = int8_quantized_model.to(device)\n",
        "\n",
        "def evaluate_int8_vgg11(model, dataloader):\n",
        "    \"\"\"Evaluates the int8 quantized VGG11 model.\n",
        "\n",
        "    Args:\n",
        "        model: The int8 quantized VGG11 model.\n",
        "        dataloader: The dataloader for the evaluation data.\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model on the evaluation data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating Int8 VGG11\", leave=False):\n",
        "            images = images.to(device)  # Move images to device\n",
        "            labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "            outputs = model(images)  # Perform forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * (correct / total)\n",
        "    return accuracy\n",
        "\n",
        "# Testing with the int8 quantized model\n",
        "int8_quantized_acc = evaluate_int8_vgg11(int8_quantized_model, test_loader)\n",
        "print(\"Int8 Quantized Accuracy:\", int8_quantized_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e1q8KHCmdyG",
        "outputId": "0245516d-c333-44d1-ca3d-17c46081f721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int8 Quantized Accuracy: 61.83\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing INT8 quantized model for QAT"
      ],
      "metadata": {
        "id": "QP5hXAFpmiLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchao.quantization.prototype.qat import Int8DynActInt4WeightQATQuantizer\n",
        "\n",
        "# Quantizing the model to int 8 bit width using the torch ao apis\n",
        "# int8_quantized_model = copy.deepcopy(vgg11)\n",
        "int8_quantized_model = copy.deepcopy(finetuned_model)\n",
        "\n",
        "# Quantizing the model\n",
        "qat_quantizer = Int8DynActInt4WeightQATQuantizer()\n",
        "\n",
        "int8_quantized_model = qat_quantizer.prepare(int8_quantized_model)\n",
        "\n",
        "# Printing the model summary following quanitzation\n",
        "summary(int8_quantized_model, input_size=(1, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE3Rw1pNmlSF",
        "outputId": "028bbda6-4940-41dd-fde9-818508eccfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG                                      [1, 100]                  --\n",
              "├─Sequential: 1-1                        [1, 512, 7, 7]            --\n",
              "│    └─Conv2d: 2-1                       [1, 64, 224, 224]         (1,792)\n",
              "│    └─ReLU: 2-2                         [1, 64, 224, 224]         --\n",
              "│    └─MaxPool2d: 2-3                    [1, 64, 112, 112]         --\n",
              "│    └─Conv2d: 2-4                       [1, 128, 112, 112]        (73,856)\n",
              "│    └─ReLU: 2-5                         [1, 128, 112, 112]        --\n",
              "│    └─MaxPool2d: 2-6                    [1, 128, 56, 56]          --\n",
              "│    └─Conv2d: 2-7                       [1, 256, 56, 56]          (295,168)\n",
              "│    └─ReLU: 2-8                         [1, 256, 56, 56]          --\n",
              "│    └─Conv2d: 2-9                       [1, 256, 56, 56]          (590,080)\n",
              "│    └─ReLU: 2-10                        [1, 256, 56, 56]          --\n",
              "│    └─MaxPool2d: 2-11                   [1, 256, 28, 28]          --\n",
              "│    └─Conv2d: 2-12                      [1, 512, 28, 28]          (1,180,160)\n",
              "│    └─ReLU: 2-13                        [1, 512, 28, 28]          --\n",
              "│    └─Conv2d: 2-14                      [1, 512, 28, 28]          (2,359,808)\n",
              "│    └─ReLU: 2-15                        [1, 512, 28, 28]          --\n",
              "│    └─MaxPool2d: 2-16                   [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-17                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-18                        [1, 512, 14, 14]          --\n",
              "│    └─Conv2d: 2-19                      [1, 512, 14, 14]          (2,359,808)\n",
              "│    └─ReLU: 2-20                        [1, 512, 14, 14]          --\n",
              "│    └─MaxPool2d: 2-21                   [1, 512, 7, 7]            --\n",
              "├─AdaptiveAvgPool2d: 1-2                 [1, 512, 7, 7]            --\n",
              "├─Sequential: 1-3                        [1, 100]                  --\n",
              "│    └─Linear: 2-22                      [1, 4096]                 102,764,544\n",
              "│    └─ReLU: 2-23                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-24                     [1, 4096]                 --\n",
              "│    └─Linear: 2-25                      [1, 4096]                 16,781,312\n",
              "│    └─ReLU: 2-26                        [1, 4096]                 --\n",
              "│    └─Dropout: 2-27                     [1, 4096]                 --\n",
              "│    └─Linear: 2-28                      [1, 100]                  409,700\n",
              "==========================================================================================\n",
              "Total params: 129,176,036\n",
              "Trainable params: 119,947,364\n",
              "Non-trainable params: 9,228,672\n",
              "Total mult-adds (G): 7.61\n",
              "==========================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 59.47\n",
              "Params size (MB): 516.70\n",
              "Estimated Total Size (MB): 576.78\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Finetuning the model"
      ],
      "metadata": {
        "id": "Pmp0B4mRmqB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to the device (e.g., GPU if available)\n",
        "int8_quantized_model = int8_quantized_model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(int8_quantized_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "num_epochs = 3  # Adjust based on performance needs\n",
        "for epoch in range(num_epochs):\n",
        "    int8_quantized_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = int8_quantized_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Print average loss and training accuracy for the epoch\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Evaluate on the test set and print test accuracy\n",
        "    int8_quantized_model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = int8_quantized_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.2f}%\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxPZdfxrmrXB",
        "outputId": "544ea451-912f-402d-bf31-043e13240448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Average Loss: 1.8824, Training Accuracy: 48.34%\n",
            "Epoch [1/3], Test Accuracy: 58.53%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3], Average Loss: 1.3983, Training Accuracy: 59.91%\n",
            "Epoch [2/3], Test Accuracy: 62.50%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3], Average Loss: 1.0341, Training Accuracy: 69.56%\n",
            "Epoch [3/3], Test Accuracy: 64.59%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting the model to the quantized version"
      ],
      "metadata": {
        "id": "hF-QNmESsoMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "int8_quantized_model = qat_quantizer.convert(int8_quantized_model)\n",
        "\n",
        "# Evaluating after converting the model\n",
        "int8_quantized_acc = evaluate_int8_vgg11(int8_quantized_model, test_loader)\n",
        "print(\"Int8 Quantized Accuracy:\", int8_quantized_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PvXOR3IsrR3",
        "outputId": "132b7e9c-f119-443a-9c31-7b764cc46def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int8 Quantized Accuracy: 64.59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PTQ with INT4 Bit Width"
      ],
      "metadata": {
        "id": "INcWnvdKM697"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchao.quantization.prototype.qat import Int4WeightOnlyQATQuantizer\n",
        "\n",
        "# Quantizing the model to int 8 bit width using the torch ao apis\n",
        "# int4_quantized_model = copy.deepcopy(vgg11)\n",
        "int4_quantized_model = copy.deepcopy(finetuned_model)\n",
        "\n",
        "\n",
        "int4_quantized_model = int4_quantized_model.to(torch.bfloat16)\n",
        "\n",
        "# quantize(int4_quantized_model, weights=qint4, activations=qint4)\n",
        "\n",
        "quantize_(int4_quantized_model, int4_weight_only())\n",
        "\n",
        "# # Quantizing the model\n",
        "# qat_quantizer = Int4WeightOnlyQATQuantizer()\n",
        "\n",
        "# int4_quantized_model = qat_quantizer.prepare(int4_quantized_model)\n",
        "\n",
        "# Printing the model summary following quanitzation\n",
        "# summary(int4_quantized_model, input_size=(1, 3, 224, 224), dtypes=[qint4])"
      ],
      "metadata": {
        "id": "TcwU5ic7M81B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluating the model after quantization"
      ],
      "metadata": {
        "id": "QdSGbnhgM_Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# int4_quantized_model = qat_quantizer.convert(int4_quantized_model)\n",
        "int4_quantized_model = int4_quantized_model.to(device)\n",
        "\n",
        "def evaluate_int4_vgg11(model, dataloader):\n",
        "    \"\"\"Evaluates the int4 quantized VGG11 model.\n",
        "\n",
        "    Args:\n",
        "        model: The int4 quantized VGG11 model.\n",
        "        dataloader: The dataloader for the evaluation data.\n",
        "\n",
        "    Returns:\n",
        "        The accuracy of the model on the evaluation data.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(dataloader, desc=\"Evaluating Int4 VGG11\", leave=False):\n",
        "            images = images.to(device)  # Move images to device\n",
        "            labels = labels.to(device)  # Move labels to device\n",
        "\n",
        "            images = images.to(torch.bfloat16)\n",
        "\n",
        "            outputs = model(images)  # Perform forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Testing with the int4 quantized model\n",
        "int4_quantized_acc = evaluate_int4_vgg11(int4_quantized_model, test_loader)\n",
        "print(\"Int4 Quantized Accuracy:\", int4_quantized_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qW_uwjOwNBHG",
        "outputId": "0794e249-c11e-4d62-a9c2-d9ba7be2837a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int4 Quantized Accuracy: 61.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preparing the Int 4 model for QAT"
      ],
      "metadata": {
        "id": "TKURe--yqX7P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchao.quantization.prototype.qat import Int4WeightOnlyQATQuantizer\n",
        "\n",
        "int4_quantized_model = copy.deepcopy(finetuned_model)\n",
        "\n",
        "\n",
        "int4_quantized_model = int4_quantized_model.to(torch.bfloat16)\n",
        "\n",
        "# Quantizing the model\n",
        "qat_quantizer = Int4WeightOnlyQATQuantizer()\n",
        "\n",
        "int4_quantized_model = qat_quantizer.prepare(int4_quantized_model)\n"
      ],
      "metadata": {
        "id": "CawmCWBdqXjH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QAT with the INT4 model"
      ],
      "metadata": {
        "id": "J38NMLvvpplk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to the device (e.g., GPU if available)\n",
        "int4_quantized_model = int4_quantized_model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(int4_quantized_model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "num_epochs = 3  # Adjust based on performance needs\n",
        "for epoch in range(num_epochs):\n",
        "    int4_quantized_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        images = images.to(torch.bfloat16)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = int4_quantized_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    # Print average loss and training accuracy for the epoch\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Evaluate on the test set and print test accuracy\n",
        "    int4_quantized_model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            images = images.to(torch.bfloat16)\n",
        "            outputs = int4_quantized_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "            total_test += labels.size(0)\n",
        "\n",
        "    test_accuracy = 100 * correct_test / total_test\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy: {test_accuracy:.2f}%\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjpU32xAprUE",
        "outputId": "304951b7-932c-4b74-c566-5b4798bfd4e6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Average Loss: 1.8733, Training Accuracy: 48.61%\n",
            "Epoch [1/3], Test Accuracy: 59.10%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3], Average Loss: 1.4090, Training Accuracy: 59.99%\n",
            "Epoch [2/3], Test Accuracy: 62.63%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3], Average Loss: 1.0560, Training Accuracy: 68.69%\n",
            "Epoch [3/3], Test Accuracy: 64.90%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting the model following finetuning"
      ],
      "metadata": {
        "id": "BZ-3LdAlTDF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "int4_quantized_model = qat_quantizer.convert(int4_quantized_model)\n",
        "\n",
        "# Evaluating after converting the model\n",
        "int4_quantized_acc = evaluate_int4_vgg11(int4_quantized_model, test_loader)\n",
        "print(\"Int8 Quantized Accuracy:\", int4_quantized_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjqRpol3TFS9",
        "outputId": "ec4aefdc-fb9e-4f02-b475-581824a72a83"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                        "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Int8 Quantized Accuracy: 64.89\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}